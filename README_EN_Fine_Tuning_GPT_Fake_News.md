# Fine-Tuning GPT for Fake News Generation

The Jupyter notebook `Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ_GPT,_Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ_Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ñ‹Ñ…_Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹.ipynb` demonstrates **fine-tuning a GPT-based language model** on a custom dataset of news articles to generate realistic-looking fake news.

## ğŸ§  Fine-Tuning GPT for Fake News Generation

- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1a2_OfmguNoDJ6v8R3shKLZK6B1k1zxYZ?usp=sharing)

## ğŸ¯ Project Goals
- Understand the **fine-tuning process** for GPT models on domain-specific data.  
- Demonstrate changes in model behavior after training.  
- Build a simple fake news generator capable of producing plausible short news stories.  

## ğŸ§  Workflow
1. **Data preparation** â€” collect and clean a dataset of real and synthetic news texts.  
2. **Text preprocessing** â€” tokenization, stopword removal, normalization.  
3. **Model fine-tuning** â€” train a pre-trained GPT model (`GPT-2` / `distilgpt2`) using the `transformers` library.  
4. **Text generation** â€” generate new news examples using the fine-tuned model.  
5. **Evaluation** â€” compare generated texts with the original ones by structure and fluency.  

## ğŸ§° Libraries
- `transformers`, `datasets`, `torch`  
- `pandas`, `numpy`, `matplotlib`  
- `tqdm`, `re`, `nltk`  

## ğŸ“Š Results
- The model was successfully fine-tuned on the custom dataset.  
- Generated texts mimic real news style and syntax.  
- The project demonstrates practical GPT fine-tuning for NLP tasks.  

## ğŸ“ Project Structure
```
â”œâ”€ data/
â”‚  â”œâ”€ train.csv
â”‚  â”œâ”€ valid.csv
â”‚  â””â”€ tokenizer/
â”‚
â”œâ”€ Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ_GPT,_Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ_Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ñ‹Ñ…_Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹.ipynb
â””â”€ README_EN.md
```

## ğŸ“ Output Data
- Generated news texts produced by the fine-tuned GPT model.  
- Training logs and metrics.
